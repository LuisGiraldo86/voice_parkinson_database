{
  "_id": {
    "$oid": "68ecf00c6c43da02c1689360"
  },
  "title": "Unveiling Interpretability in Self-Supervised Speech Representations for Parkinson's Diagnosis",
  "authors": [
    {
      "name": "David Gimeno-Gomez",
      "affiliations": [
        {
          "institution": "Universitat Politecnica de Valencia",
          "country": "Spain"
        }
      ]
    },
    {
      "name": "Catarina Botelho",
      "affiliations": [
        {
          "institution": "Universidade de Lisboa",
          "country": "Portugal"
        }
      ]
    },
    {
      "name": "Anna Pompili",
      "affiliations": [
        {
          "institution": "Universidade de Lisboa",
          "country": "Portugal"
        }
      ]
    },
    {
      "name": "Alberto Abad",
      "affiliations": [
        {
          "institution": "Universidade de Lisboa",
          "country": "Portugal"
        }
      ]
    },
    {
      "name": "Carlos-D. Mart√≠nez-Hinarejos",
      "affiliations": [
        {
          "institution": "Universitat Politecnica de Valencia",
          "country": "Spain"
        }
      ]
    }
  ],
  "publication_type": "Journal",
  "journal": "IEEE Journal of Selected Topics in Signal Processing",
  "year": 2025,
  "doi": "10.1109/JSTSP.2025.3539845",
  "abstract": "Recent works in pathological speech analysis have increasingly relied on powerful self-supervised speech representations, leading to promising results. However, the complex, black-box nature of these embeddings and the limited research on their interpretability significantly restrict their adoption for clinical diagnosis. To address this gap, we propose a novel, interpretable framework specifically designed to support Parkinson's Disease (PD) diagnosis. Through the design of simple yet effective cross-attention mechanisms for both embedding- and temporal-level analysis, the proposed framework offers interpretability from two distinct but complementary perspectives. Experimental findings across five well-established speech benchmarks for PD detection demonstrate the framework's capability to identify meaningful speech patterns within self-supervised representations for a wide range of assessment tasks. Fine-grained temporal analyses further underscore its potential to enhance the interpretability of deep-learning pathological speech models, paving the way for the development of more transparent, trustworthy, and clinically applicable computer-assisted diagnosis systems in this domain. Moreover, in terms of classification accuracy, our method achieves results competitive with state-of-the-art approaches, while also demonstrating robustness in cross-lingual scenarios when applied to spontaneous speech production.",
  "ml_approaches": [
    {
      "algorithm": "Cross-Attention-based Fusion Model",
      "framework": "Python",
      "input_speech": "vowels",
      "scaling": [
        "Standardization"
      ],
      "feature_extraction": [
        "XLR-S Wav2Vec2.0",
        "Handcrafted Informed Features"
      ],
      "feature_selection": {
        "methods": [
          "Cross-attention weighting"
        ]
      },
      "validation": "5-fold cross-validation",
      "results": {
        "f1_score": 0.534
      }
    },
    {
      "algorithm": "Cross-Attention-based Fusion Model",
      "framework": "Python",
      "input_speech": "words",
      "scaling": [
        "Standardization"
      ],
      "feature_extraction": [
        "XLR-S Wav2Vec2.0",
        "Handcrafted Informed Features"
      ],
      "feature_selection": {
        "methods": [
          "Cross-attention weighting"
        ]
      },
      "validation": "5-fold cross-validation",
      "results": {
        "f1_score": 0.691
      }
    },
    {
      "algorithm": "Cross-Attention-based Fusion Model",
      "framework": "Python",
      "input_speech": "ddk",
      "scaling": [
        "Standardization"
      ],
      "feature_extraction": [
        "XLR-S Wav2Vec2.0",
        "Handcrafted Informed Features"
      ],
      "feature_selection": {
        "methods": [
          "Cross-attention weighting"
        ]
      },
      "validation": "5-fold cross-validation",
      "results": {
        "f1_score": 0.677
      }
    },
    {
      "algorithm": "Cross-Attention-based Fusion Model",
      "framework": "Python",
      "input_speech": "sentences",
      "scaling": [
        "Standardization"
      ],
      "feature_extraction": [
        "XLR-S Wav2Vec2.0",
        "Handcrafted Informed Features"
      ],
      "feature_selection": {
        "methods": [
          "Cross-attention weighting"
        ]
      },
      "validation": "5-fold cross-validation",
      "results": {
        "f1_score": 0.706
      }
    },
    {
      "algorithm": "Cross-Attention-based Fusion Model",
      "framework": "Python",
      "input_speech": "read text",
      "scaling": [
        "Standardization"
      ],
      "feature_extraction": [
        "XLR-S Wav2Vec2.0",
        "Handcrafted Informed Features"
      ],
      "feature_selection": {
        "methods": [
          "Cross-attention weighting"
        ]
      },
      "validation": "5-fold cross-validation",
      "results": {
        "f1_score": 0.768
      }
    },
    {
      "algorithm": "Cross-Attention-based Fusion Model",
      "framework": "Python",
      "input_speech": "monologue",
      "scaling": [
        "Standardization"
      ],
      "feature_extraction": [
        "XLR-S Wav2Vec2.0",
        "Handcrafted Informed Features"
      ],
      "feature_selection": {
        "methods": [
          "Cross-attention weighting"
        ]
      },
      "validation": "5-fold cross-validation",
      "results": {
        "f1_score": 0.7
      }
    }
  ],
  "source_dataset": [
    {
      "name": "FralusoPark",
      "size": 100,
      "pd_patients": 65,
      "controls": 75,
      "url": null,
      "doi": null
    }
  ],
  "target_dataset": [
    {
      "name": "FralusoPark",
      "size": 100,
      "pd_patients": 65,
      "controls": 75,
      "url": null,
      "doi": null
    }
  ],
  "study_id": "10.1109/JSTSP.2025.3539845_5"
}