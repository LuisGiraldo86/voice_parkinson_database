{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b3ed569",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from plotly.subplots import make_subplots\n",
    "from pymongo import MongoClient\n",
    "from collections import Counter, defaultdict\n",
    "import os\n",
    "import warnings\n",
    "import sys\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set visualization style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "library_path = os.path.abspath('..')\n",
    "if library_path not in sys.path:\n",
    "    sys.path.append(library_path)\n",
    "\n",
    "PLOTS_PATH = os.path.join(library_path, 'plots')\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(f\"Current working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0542f04b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to MongoDB\n",
    "client = MongoClient(\"mongodb://localhost:27017/\")\n",
    "db = client[\"Diagnosis_Severity_PD_Voice\"]\n",
    "collection = db[\"studies\"]\n",
    "\n",
    "print(\"üîÑ Loading studies from MongoDB...\")\n",
    "fields_to_extract = {\"doi\": 1, \"source_dataset\": 1, \"target_dataset\": 1, '_id':0}  # 1 = include, 0 = exclude\n",
    "studies_cursor = collection.find({}, fields_to_extract)\n",
    "studies_list = list(studies_cursor)\n",
    "\n",
    "print(f\"üìä Total studies loaded: {len(studies_list)}\")\n",
    "print(f\"üìÑ Sample document keys: {list(studies_list[0].keys()) if studies_list else 'No documents found'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c5dd43b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore the structure of source_dataset and target_dataset fields\n",
    "print(\"üîç Analyzing dataset structure...\")\n",
    "\n",
    "if studies_list:\n",
    "    # Get a sample document to examine structure\n",
    "    sample_doc = studies_list[0]\n",
    "    \n",
    "    # Count documents with source_dataset and target_dataset\n",
    "    docs_with_source = sum(1 for doc in studies_list if 'source_dataset' in doc and doc['source_dataset'])\n",
    "    docs_with_target = sum(1 for doc in studies_list if 'target_dataset' in doc and doc['target_dataset'])\n",
    "    \n",
    "    print(f\"\\nüìä Dataset field coverage:\")\n",
    "    print(f\"  Documents with source_dataset: {docs_with_source}/{len(studies_list)} ({docs_with_source/len(studies_list)*100:.1f}%)\")\n",
    "    print(f\"  Documents with target_dataset: {docs_with_target}/{len(studies_list)} ({docs_with_target/len(studies_list)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c83985c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze the datasets used in studies\n",
    "print(\"üìä Analyzing datasets used in studies...\")\n",
    "\n",
    "# Collect all source and target datasets\n",
    "all_source_datasets = []\n",
    "all_target_datasets = []\n",
    "dataset_usage = defaultdict(int)\n",
    "\n",
    "for doc in studies_list:\n",
    "    # Process source datasets\n",
    "    if 'source_dataset' in doc and doc['source_dataset']:\n",
    "        for dataset in doc['source_dataset']:\n",
    "            all_source_datasets.append(dataset)\n",
    "            if 'name' in dataset:\n",
    "                dataset_usage[dataset['name']] += 1\n",
    "    \n",
    "    # Process target datasets\n",
    "    if 'target_dataset' in doc and doc['target_dataset']:\n",
    "        for dataset in doc['target_dataset']:\n",
    "            all_target_datasets.append(dataset)\n",
    "\n",
    "print(f\"\\nüìà Dataset statistics:\")\n",
    "print(f\"  Total source dataset entries: {len(all_source_datasets)}\")\n",
    "print(f\"  Total target dataset entries: {len(all_target_datasets)}\")\n",
    "print(f\"  Unique dataset names: {len(dataset_usage)}\")\n",
    "\n",
    "# Show most commonly used datasets\n",
    "print(f\"\\nüîù Top 10 most used datasets:\")\n",
    "top_datasets = sorted(dataset_usage.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "for i, (name, count) in enumerate(top_datasets, 1):\n",
    "    print(f\"  {i:2d}. {name}: {count} studies\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8aa1ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "global_dataset_usage = defaultdict(int)\n",
    "all_source_datasets = defaultdict(int)\n",
    "all_target_datasets = defaultdict(int)\n",
    "\n",
    "for doc in studies_list:\n",
    "\n",
    "    study_source = []\n",
    "    study_target = []\n",
    "\n",
    "    # Process source datasets\n",
    "    if 'source_dataset' in doc and doc['source_dataset']:\n",
    "        for dataset in doc['source_dataset']:\n",
    "            study_source.append(dataset['name'])\n",
    "            all_source_datasets[dataset['name']] += 1\n",
    "    \n",
    "    # Process target datasets\n",
    "    if 'target_dataset' in doc and doc['target_dataset']:\n",
    "        for dataset in doc['target_dataset']:\n",
    "            study_target.append(dataset['name'])\n",
    "            all_target_datasets[dataset['name']] += 1\n",
    "    \n",
    "    # Count each dataset once per study\n",
    "    unique_dataset = set(study_source + study_target)\n",
    "    for dataset_name in unique_dataset:\n",
    "        global_dataset_usage[dataset_name] += 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6fe9f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nüìà Dataset statistics:\")\n",
    "print(f\"\\nüîù Top 10 most used datasets:\")\n",
    "top_datasets = sorted(global_dataset_usage.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "for i, (name, count) in enumerate(top_datasets, 1):\n",
    "    print(f\"  {i:2d}. {name}: {count} studies\")\n",
    "\n",
    "print(f\"\\nüîù Top 5 most used source datasets:\")\n",
    "top_datasets = sorted(all_source_datasets.items(), key=lambda x: x[1], reverse=True)[:5]\n",
    "for i, (name, count) in enumerate(top_datasets, 1):\n",
    "    print(f\"  {i:2d}. {name}: {count} studies\")\n",
    "\n",
    "print(f\"\\nüîù Top 5 most used target datasets:\")\n",
    "top_datasets = sorted(all_target_datasets.items(), key=lambda x: x[1], reverse=True)[:5]\n",
    "for i, (name, count) in enumerate(top_datasets, 1):\n",
    "    print(f\"  {i:2d}. {name}: {count} studies\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "972891db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "def plot_top_datasets(dataset_counter, title, top_n=10):\n",
    "    top = Counter(dataset_counter).most_common(top_n)\n",
    "    names, counts = zip(*top)\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.barh(names, counts, color='skyblue')\n",
    "    plt.xlabel('Count')\n",
    "    plt.title(title)\n",
    "    plt.gca().invert_yaxis()  # largest on top\n",
    "    plt.show()\n",
    "\n",
    "# Example usage:\n",
    "plot_top_datasets(all_source_datasets, \"Top Source Datasets\")\n",
    "plot_top_datasets(all_target_datasets, \"Top Target Datasets\")\n",
    "plot_top_datasets(global_dataset_usage, \"Dataset Usage Across Studies\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ccf2ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'dataset': list(set(all_source_datasets) | set(all_target_datasets)),\n",
    "    'source_count': [all_source_datasets[d] for d in set(all_source_datasets) | set(all_target_datasets)],\n",
    "    'target_count': [all_target_datasets[d] for d in set(all_source_datasets) | set(all_target_datasets)],\n",
    "})\n",
    "\n",
    "df = df.sort_values('source_count', ascending=False).head(10)\n",
    "\n",
    "df.plot(\n",
    "    x='dataset', kind='bar', stacked=False, figsize=(12,6),\n",
    "    color=['skyblue', 'salmon'], xlabel='Dataset', ylabel='Count', title='Top 10 Datasets: Source vs Target'\n",
    ")\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08fd0b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "fields_to_extract = {\"doi\": 1, \"source_dataset\": 1, \"target_dataset\": 1, '_id':0}  # 1 = include, 0 = exclude\n",
    "documents = collection.find({}, fields_to_extract)\n",
    "\n",
    "unique_dois = collection.distinct(\"doi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb088510",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_by_doi = []\n",
    "for doi in unique_dois:\n",
    "    doi_docs = collection.find({\"doi\": doi}, projection=fields_to_extract)\n",
    "    summary_dict = {'doi': doi, 'datasets': set()}\n",
    "\n",
    "    for doc in doi_docs:\n",
    "        datasets = [\n",
    "            d['name']\n",
    "            for field in ('source_dataset', 'target_dataset')\n",
    "            if field in doc and doc[field]\n",
    "            for d in doc[field]\n",
    "        ]\n",
    "        summary_dict['datasets'].update(datasets)\n",
    "\n",
    "    summary_dict['datasets'] = list(summary_dict['datasets'])\n",
    "    dataset_by_doi.append(summary_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e04ef543",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(dataset_by_doi)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c54ad6c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_exploded = df.explode('datasets')\n",
    "df_exploded.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae5879d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_counts =df_exploded.groupby('datasets', as_index=False).size().sort_values(by='size', ascending=False)\n",
    "dataset_counts.columns = ['dataset', 'count']\n",
    "dataset_counts['percentage'] = np.round(dataset_counts['count'] / df['doi'].nunique() * 100, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b0e88d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_counts = dataset_counts.reset_index(drop=True)\n",
    "dataset_counts.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c708b00e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['num_datasets'] = df['datasets'].apply(len)\n",
    "datasets_by_paper = df.groupby('num_datasets').size().reset_index(name='num_papers')\n",
    "datasets_by_paper['percentages'] = np.round(datasets_by_paper['num_papers'] / df.shape[0] * 100, 2)\n",
    "datasets_by_paper"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "voice-db-Uw6PKE_M-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
