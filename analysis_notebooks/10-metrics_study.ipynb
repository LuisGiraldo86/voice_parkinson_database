{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "402d5a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pymongo import MongoClient\n",
    "import os\n",
    "import warnings\n",
    "import sys\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set visualization style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "library_path = os.path.abspath('..')\n",
    "if library_path not in sys.path:\n",
    "    sys.path.append(library_path)\n",
    "\n",
    "PLOTS_PATH = os.path.join(library_path, 'plots')\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(f\"Current working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d9f5fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to MongoDB\n",
    "client = MongoClient(\"mongodb://localhost:27017/\")\n",
    "db = client[\"Diagnosis_Severity_PD_Voice\"]\n",
    "collection = db[\"studies\"]\n",
    "\n",
    "print(\"ðŸ”„ Loading studies from MongoDB...\")\n",
    "fields_to_extract = {\n",
    "    \"doi\"             : 1, \n",
    "    \"year\"            : 1, \n",
    "    \"study_id\"        : 1,\n",
    "    \"ml_approaches\"   : 1,\n",
    "    \"problem\"         : 1,\n",
    "    \"ml_problem_type\" : 1,\n",
    "    '_id'             : 0\n",
    "}  # 1 = include, 0 = exclude\n",
    "studies_cursor = collection.find({}, fields_to_extract)\n",
    "studies_list = list(studies_cursor)\n",
    "\n",
    "\n",
    "print(f\"ðŸ“Š Total studies loaded: {len(studies_list)}\")\n",
    "print(f\"ðŸ“„ Sample document keys: {list(studies_list[0].keys()) if studies_list else 'No documents found'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b16631a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics_experiment(experiment_list: list):\n",
    "    \n",
    "    metric_list = []\n",
    "\n",
    "    for experiment in experiment_list:\n",
    "\n",
    "        metrics = experiment.get('results', {})\n",
    "        model = experiment.get('algorithm','')\n",
    "        validation = experiment.get('validation', '')\n",
    "        metric_list.append(tuple((model, metrics, validation)))\n",
    "\n",
    "    return metric_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c4554da",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_df = pd.DataFrame(studies_list)\n",
    "\n",
    "experiment_df['alg_metrics'] = experiment_df['ml_approaches'].apply(get_metrics_experiment)\n",
    "experiment_df = experiment_df.explode('alg_metrics').reset_index(drop=True)\n",
    "experiment_df['algorithm'] = experiment_df['alg_metrics'].apply(lambda x: x[0])\n",
    "experiment_df['metrics'] = experiment_df['alg_metrics'].apply(lambda x: x[1])\n",
    "experiment_df['validation'] = experiment_df['alg_metrics'].apply(lambda x: x[2])\n",
    "experiment_df = experiment_df.drop(columns=['alg_metrics'])\n",
    "experiment_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f06749b",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_df['metric_used'] = experiment_df['metrics'].apply(lambda x: list(set(list(x.keys()) if isinstance(x, dict) else [])))\n",
    "experiment_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3de4221",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_df['validation'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "312d7841",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_val_df = experiment_df.groupby(by=['doi', 'problem'], as_index=False).agg({\n",
    "    'metric_used': list,\n",
    "    'validation': list,\n",
    "})\n",
    "metric_val_df['metrics_used'] = metric_val_df['metric_used'].apply(lambda x: set([item for sublist in x for item in sublist]))\n",
    "metric_val_df['num_metrics'] = metric_val_df['metrics_used'].apply(lambda x: len(x))\n",
    "metric_val_df['validation'] = metric_val_df['validation'].apply(lambda x: list(set(x)))\n",
    "metric_val_df['validation'] = metric_val_df['validation'].apply(lambda x: [val for val in x if val != ''])\n",
    "metric_val_df['validation'] = metric_val_df['validation'].apply(lambda x: [val for val in x if val is not None])\n",
    "metric_val_df['num_validation'] = metric_val_df['validation'].apply(lambda x: len(x))\n",
    "metric_val_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40615291",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_validation = metric_val_df.groupby(by='doi', as_index=False).agg({\n",
    "    'num_validation': 'sum'})\n",
    "num_validation['num_validation'].value_counts(normalize=True)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a6572b",
   "metadata": {},
   "outputs": [],
   "source": [
    "vals_df = metric_val_df.groupby(by='doi', as_index=False).agg({\n",
    "    'validation': list})\n",
    "vals_df['validation_used'] = vals_df['validation'].apply(lambda x: list(set([item for sublist in x for item in sublist])))\n",
    "vals_df['num_validation_used'] = vals_df['validation_used'].apply(lambda x: len(x))\n",
    "vals_df = vals_df.drop(columns=['validation'], inplace=False)\n",
    "vals_df['num_validation_used'].value_counts(normalize=True)*100\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d9c3a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df = vals_df.explode('validation_used').reset_index(drop=True)\n",
    "temp = temp_df['validation_used'].value_counts(normalize=True).reset_index()\n",
    "temp.columns = ['validation', 'proportion']\n",
    "temp['proportion'] = np.round(temp['proportion']*100, 2)\n",
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a5541fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_metrics = metric_val_df.groupby(by='doi', as_index=False).agg({\n",
    "    'num_metrics': 'sum'})\n",
    "\n",
    "print(f\"Max number of metrics in a paper: {num_metrics['num_metrics'].max()}\")\n",
    "print(f\"Min number of metrics in a paper: {num_metrics['num_metrics'].min()}\")\n",
    "print(f\"Median number of metrics in a paper: {num_metrics['num_metrics'].median():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba04e9b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_metrics[num_metrics['num_metrics'] == num_metrics['num_metrics'].min()].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56de5cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e476487f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_str_lists(str_list):\n",
    "    if isinstance(str_list, list):\n",
    "        return ', '.join([s.strip() for s in str_list])\n",
    "    return str_list\n",
    "\n",
    "experiment_df['for_group_ml_type'] = experiment_df['ml_problem_type'].apply(lambda x: concat_str_lists(x))\n",
    "experiment_df.groupby(by=['problem', 'for_group_ml_type'], as_index=False).agg(\n",
    "    doi_unique=('doi', 'nunique'),\n",
    "    doi_count=('doi', 'count')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93220516",
   "metadata": {},
   "outputs": [],
   "source": [
    "exploded_metrics  =(experiment_df.assign(kv=experiment_df[\"metrics\"].apply(lambda d: list(d.items())))\n",
    "   .explode(\"kv\")\n",
    "   .assign(metric=lambda x: x[\"kv\"].str[0],\n",
    "           value=lambda x: x[\"kv\"].str[1])\n",
    "   .drop(columns=[\"kv\"])\n",
    ")\n",
    "exploded_metrics = exploded_metrics.drop(columns=['year', 'ml_approaches', 'metrics', 'validation'], inplace=False)\n",
    "exploded_metrics = exploded_metrics.explode('ml_problem_type').reset_index(drop=True)\n",
    "exploded_metrics['ml_problem_type'] = exploded_metrics['ml_problem_type'].apply(lambda x: x.split(':')[0] if isinstance(x, str) else x)\n",
    "exploded_metrics.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c7f7c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_metrics = exploded_metrics[exploded_metrics['ml_problem_type'] == 'Classification'].reset_index(drop=True)\n",
    "class_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d0e789",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of metrics used in Classification problems: {class_metrics['metric'].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45db1652",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_count =class_metrics['metric'].value_counts(normalize=True).reset_index()\n",
    "metric_count.columns = ['metric', 'percent']\n",
    "metric_count['percent'] = np.round(metric_count['percent']*100, 2)\n",
    "metric_count.head(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e65d7d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_count =class_metrics['metric'].value_counts(normalize=False).reset_index()\n",
    "metric_count.columns = ['metric', 'count']\n",
    "metric_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a57284",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_by_doi = class_metrics.groupby(by='metric', as_index=False).agg({\n",
    "    'doi': 'nunique'\n",
    "})\n",
    "metric_by_doi.sort_values(by='doi', ascending=False, inplace=True)\n",
    "metric_by_doi.rename(columns={'doi': 'num_doi'}, inplace=True)\n",
    "metric_by_doi['percentage_doi'] = np.round((metric_by_doi['num_doi'] / class_metrics['doi'].nunique()) * 100, 2)\n",
    "metric_by_doi.head(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35dd55f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Mean Accuracy reported in Classification problems: {class_metrics[class_metrics['metric'] == 'accuracy']['value'].mean():.2f}\")\n",
    "print(f\"Std Deviation of Accuracy reported in Classification problems: {class_metrics[class_metrics['metric'] == 'accuracy']['value'].std():.2f}\")\n",
    "print(f\"Median Accuracy reported in Classification problems: {class_metrics[class_metrics['metric'] == 'accuracy']['value'].median():.2f}\")\n",
    "print(f'Min Accuracy reported in Classification problems: {class_metrics[class_metrics['metric'] == 'accuracy']['value'].min():.2f}')\n",
    "print(f'Max Accuracy reported in Classification problems: {class_metrics[class_metrics['metric'] == 'accuracy']['value'].max():.2f}')\n",
    "print(f\"First Quartile of Accuracy reported in Classification problems: {class_metrics[class_metrics['metric'] == 'accuracy']['value'].quantile(0.25):.2f}\")\n",
    "print(f\"Third Quartile of Accuracy reported in Classification problems: {class_metrics[class_metrics['metric'] == 'accuracy']['value'].quantile(0.75):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd294db",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Mean Recall reported in Classification problems: {class_metrics[class_metrics['metric'] == 'recall']['value'].mean():.2f}\")\n",
    "print(f\"Std Deviation of Recall reported in Classification problems: {class_metrics[class_metrics['metric'] == 'recall']['value'].std():.2f}\")\n",
    "print(f\"Median Recall reported in Classification problems: {class_metrics[class_metrics['metric'] == 'recall']['value'].median():.2f}\")\n",
    "print(f'Min Recall reported in Classification problems: {class_metrics[class_metrics['metric'] == 'recall']['value'].min():.2f}')\n",
    "print(f'Max Recall reported in Classification problems: {class_metrics[class_metrics['metric'] == 'recall']['value'].max():.2f}')\n",
    "print(f\"First Quartile of Recall reported in Classification problems: {class_metrics[class_metrics['metric'] == 'recall']['value'].quantile(0.25):.2f}\")\n",
    "print(f\"Third Quartile of Recall reported in Classification problems: {class_metrics[class_metrics['metric'] == 'recall']['value'].quantile(0.75):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a659890",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Mean F1-score reported in Classification problems: {class_metrics[class_metrics['metric'] == 'f1_score']['value'].mean():.2f}\")\n",
    "print(f\"Std Deviation of F1-score reported in Classification problems: {class_metrics[class_metrics['metric'] == 'f1_score']['value'].std():.2f}\")\n",
    "print(f\"Median F1-score reported in Classification problems: {class_metrics[class_metrics['metric'] == 'f1_score']['value'].median():.2f}\")\n",
    "print(f'Min F1-score reported in Classification problems: {class_metrics[class_metrics['metric'] == 'f1_score']['value'].min():.2f}')\n",
    "print(f'Max F1-score reported in Classification problems: {class_metrics[class_metrics['metric'] == 'f1_score']['value'].max():.2f}')\n",
    "print(f\"First Quartile of F1-score reported in Classification problems: {class_metrics[class_metrics['metric'] == 'f1_score']['value'].quantile(0.25):.2f}\")\n",
    "print(f\"Third Quartile of F1-score reported in Classification problems: {class_metrics[class_metrics['metric'] == 'f1_score']['value'].quantile(0.75):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff3296f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Mean precision reported in Classification problems: {class_metrics[class_metrics['metric'] == 'precision']['value'].mean():.2f}\")\n",
    "print(f\"Std Deviation of precision reported in Classification problems: {class_metrics[class_metrics['metric'] == 'precision']['value'].std():.2f}\")\n",
    "print(f\"Median precision reported in Classification problems: {class_metrics[class_metrics['metric'] == 'precision']['value'].median():.2f}\")\n",
    "print(f'Min precision reported in Classification problems: {class_metrics[class_metrics['metric'] == 'precision']['value'].min():.2f}')\n",
    "print(f'Max precision reported in Classification problems: {class_metrics[class_metrics['metric'] == 'precision']['value'].max():.2f}')\n",
    "print(f\"First Quartile of precision reported in Classification problems: {class_metrics[class_metrics['metric'] == 'precision']['value'].quantile(0.25):.2f}\")\n",
    "print(f\"Third Quartile of precision reported in Classification problems: {class_metrics[class_metrics['metric'] == 'precision']['value'].quantile(0.75):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cbe6a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Mean Specificity reported in Classification problems: {class_metrics[class_metrics['metric'] == 'specificity']['value'].mean():.2f}\")\n",
    "print(f\"Std Deviation of Specificity reported in Classification problems: {class_metrics[class_metrics['metric'] == 'specificity']['value'].std():.2f}\")\n",
    "print(f\"Median Specificity reported in Classification problems: {class_metrics[class_metrics['metric'] == 'specificity']['value'].median():.2f}\")\n",
    "print(f'Min Specificity reported in Classification problems: {class_metrics[class_metrics['metric'] == 'specificity']['value'].min():.2f}')\n",
    "print(f'Max Specificity reported in Classification problems: {class_metrics[class_metrics['metric'] == 'specificity']['value'].max():.2f}')\n",
    "print(f\"First Quartile of Specificity reported in Classification problems: {class_metrics[class_metrics['metric'] == 'specificity']['value'].quantile(0.25):.2f}\")\n",
    "print(f\"Third Quartile of Specificity reported in Classification problems: {class_metrics[class_metrics['metric'] == 'specificity']['value'].quantile(0.75):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43821b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Mean ROC-AUC reported in Classification problems: {class_metrics[class_metrics['metric'] == 'auc']['value'].mean():.2f}\")\n",
    "print(f\"Std Deviation of ROC-AUC reported in Classification problems: {class_metrics[class_metrics['metric'] == 'auc']['value'].std():.2f}\")\n",
    "print(f\"Median ROC-AUC reported in Classification problems: {class_metrics[class_metrics['metric'] == 'auc']['value'].median():.2f}\")\n",
    "print(f'Min ROC-AUC reported in Classification problems: {class_metrics[class_metrics['metric'] == 'auc']['value'].min():.2f}')\n",
    "print(f'Max ROC-AUC reported in Classification problems: {class_metrics[class_metrics['metric'] == 'auc']['value'].max():.2f}')\n",
    "print(f\"First Quartile of ROC-AUC reported in Classification problems: {class_metrics[class_metrics['metric'] == 'auc']['value'].quantile(0.25):.2f}\")\n",
    "print(f\"Third Quartile of ROC-AUC reported in Classification problems: {class_metrics[class_metrics['metric'] == 'auc']['value'].quantile(0.75):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "410b99a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Mean MCC reported in Classification problems: {class_metrics[class_metrics['metric'] == 'mcc']['value'].mean():.2f}\")\n",
    "print(f\"Std Deviation of MCC reported in Classification problems: {class_metrics[class_metrics['metric'] == 'mcc']['value'].std():.2f}\")\n",
    "print(f\"Median MCC reported in Classification problems: {class_metrics[class_metrics['metric'] == 'mcc']['value'].median():.2f}\")\n",
    "print(f'Min MCC reported in Classification problems: {class_metrics[class_metrics['metric'] == 'mcc']['value'].min():.2f}')\n",
    "print(f'Max MCC reported in Classification problems: {class_metrics[class_metrics['metric'] == 'mcc']['value'].max():.2f}')\n",
    "print(f\"First Quartile of MCC reported in Classification problems: {class_metrics[class_metrics['metric'] == 'mcc']['value'].quantile(0.25):.2f}\")\n",
    "print(f\"Third Quartile of MCC reported in Classification problems: {class_metrics[class_metrics['metric'] == 'mcc']['value'].quantile(0.75):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b84acc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "regression_metrics = exploded_metrics[exploded_metrics['ml_problem_type'] == 'Regression'].reset_index(drop=True)\n",
    "regression_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103a530d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of metrics used in Regression problems: {regression_metrics['metric'].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8308a62c",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_count =regression_metrics['metric'].value_counts(normalize=True).reset_index()\n",
    "metric_count.columns = ['metric', 'percent']\n",
    "metric_count['percent'] = np.round(metric_count['percent']*100, 2)\n",
    "metric_count.head(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "853f632d",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_count =regression_metrics['metric'].value_counts(normalize=False).reset_index()\n",
    "metric_count.columns = ['metric', 'count']\n",
    "metric_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e15bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_by_doi = regression_metrics.groupby(by='metric', as_index=False).agg({\n",
    "    'doi': 'nunique'\n",
    "})\n",
    "metric_by_doi.sort_values(by='doi', ascending=False, inplace=True)\n",
    "metric_by_doi.rename(columns={'doi': 'num_doi'}, inplace=True)\n",
    "metric_by_doi['percentage_doi'] = np.round((metric_by_doi['num_doi'] / regression_metrics['doi'].nunique()) * 100, 2)\n",
    "metric_by_doi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c46fa2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Mean MAE reported in Regression problems: {regression_metrics[regression_metrics['metric'] == 'mae']['value'].mean():.2f}\")\n",
    "print(f\"Std Deviation of MAE reported in Regression problems: {regression_metrics[regression_metrics['metric'] == 'mae']['value'].std():.2f}\")\n",
    "print(f\"Median MAE reported in Regression problems: {regression_metrics[regression_metrics['metric'] == 'mae']['value'].median():.2f}\")\n",
    "print(f'Min MAE reported in Regression problems: {regression_metrics[regression_metrics['metric'] == 'mae']['value'].min():.2f}')\n",
    "print(f'Max MAE reported in Regression problems: {regression_metrics[regression_metrics['metric'] == 'mae']['value'].max():.2f}')\n",
    "print(f\"First Quartile of MAE reported in Regression problems: {regression_metrics[regression_metrics['metric'] == 'mae']['value'].quantile(0.25):.2f}\")\n",
    "print(f\"Third Quartile of MAE reported in Regression problems: {regression_metrics[regression_metrics['metric'] == 'mae']['value'].quantile(0.75):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "467a2fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Mean RMSE reported in Regression problems: {regression_metrics[regression_metrics['metric'] == 'rmse']['value'].mean():.2f}\")\n",
    "print(f\"Std Deviation of RMSE reported in Regression problems: {regression_metrics[regression_metrics['metric'] == 'rmse']['value'].std():.2f}\")\n",
    "print(f\"Median RMSE reported in Regression problems: {regression_metrics[regression_metrics['metric'] == 'rmse']['value'].median():.2f}\")\n",
    "print(f'Min RMSE reported in Regression problems: {regression_metrics[regression_metrics['metric'] == 'rmse']['value'].min():.2f}')\n",
    "print(f'Max RMSE reported in Regression problems: {regression_metrics[regression_metrics['metric'] == 'rmse']['value'].max():.2f}')\n",
    "print(f\"First Quartile of RMSE reported in Regression problems: {regression_metrics[regression_metrics['metric'] == 'rmse']['value'].quantile(0.25):.2f}\")\n",
    "print(f\"Third Quartile of RMSE reported in Regression problems: {regression_metrics[regression_metrics['metric'] == 'rmse']['value'].quantile(0.75):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0477979",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Mean RÂ² reported in Regression problems: {regression_metrics[regression_metrics['metric'] == 'r_squared']['value'].mean():.2f}\")\n",
    "print(f\"Std Deviation of RÂ² reported in Regression problems: {regression_metrics[regression_metrics['metric'] == 'r_squared']['value'].std():.2f}\")\n",
    "print(f\"Median RÂ² reported in Regression problems: {regression_metrics[regression_metrics['metric'] == 'r_squared']['value'].median():.2f}\")\n",
    "print(f'Min RÂ² reported in Regression problems: {regression_metrics[regression_metrics['metric'] == 'r_squared']['value'].min():.2f}')\n",
    "print(f'Max RÂ² reported in Regression problems: {regression_metrics[regression_metrics['metric'] == 'r_squared']['value'].max():.2f}')\n",
    "print(f\"First Quartile of RÂ² reported in Regression problems: {regression_metrics[regression_metrics['metric'] == 'r_squared']['value'].quantile(0.25):.2f}\")\n",
    "print(f\"Third Quartile of RÂ² reported in Regression problems: {regression_metrics[regression_metrics['metric'] == 'r_squared']['value'].quantile(0.75):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d3d17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Mean MSE reported in Regression problems: {regression_metrics[regression_metrics['metric'] == 'mse']['value'].mean():.2f}\")\n",
    "print(f\"Std Deviation of MSE reported in Regression problems: {regression_metrics[regression_metrics['metric'] == 'mse']['value'].std():.2f}\")\n",
    "print(f\"Median MSE reported in Regression problems: {regression_metrics[regression_metrics['metric'] == 'mse']['value'].median():.2f}\")\n",
    "print(f'Min MSE reported in Regression problems: {regression_metrics[regression_metrics['metric'] == 'mse']['value'].min():.2f}')\n",
    "print(f'Max MSE reported in Regression problems: {regression_metrics[regression_metrics['metric'] == 'mse']['value'].max():.2f}')\n",
    "print(f\"First Quartile of MSE reported in Regression problems: {regression_metrics[regression_metrics['metric'] == 'mse']['value'].quantile(0.25):.2f}\")\n",
    "print(f\"Third Quartile of MSE reported in Regression problems: {regression_metrics[regression_metrics['metric'] == 'mse']['value'].quantile(0.75):.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "voice-db-Uw6PKE_M-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
