{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "402d5a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pymongo import MongoClient\n",
    "import os\n",
    "import warnings\n",
    "import sys\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set visualization style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "library_path = os.path.abspath('..')\n",
    "if library_path not in sys.path:\n",
    "    sys.path.append(library_path)\n",
    "\n",
    "PLOTS_PATH = os.path.join(library_path, 'plots')\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(f\"Current working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d9f5fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to MongoDB\n",
    "client = MongoClient(\"mongodb://localhost:27017/\")\n",
    "db = client[\"Diagnosis_Severity_PD_Voice\"]\n",
    "collection = db[\"studies\"]\n",
    "\n",
    "print(\"ðŸ”„ Loading studies from MongoDB...\")\n",
    "fields_to_extract = {\n",
    "    \"doi\"             : 1, \n",
    "    \"year\"            : 1, \n",
    "    \"study_id\"        : 1,\n",
    "    \"ml_approaches\"   : 1,\n",
    "    \"problem\"         : 1,\n",
    "    \"ml_problem_type\" : 1,\n",
    "    '_id'             : 0\n",
    "}  # 1 = include, 0 = exclude\n",
    "studies_cursor = collection.find({}, fields_to_extract)\n",
    "studies_list = list(studies_cursor)\n",
    "\n",
    "\n",
    "print(f\"ðŸ“Š Total studies loaded: {len(studies_list)}\")\n",
    "print(f\"ðŸ“„ Sample document keys: {list(studies_list[0].keys()) if studies_list else 'No documents found'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b16631a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics_experiment(experiment_list: list):\n",
    "    \n",
    "    metric_list = []\n",
    "\n",
    "    for experiment in experiment_list:\n",
    "\n",
    "        metrics = experiment.get('results', {})\n",
    "        model = experiment.get('algorithm','')\n",
    "        validation = experiment.get('validation', '')\n",
    "        metric_list.append(tuple((model, metrics, validation)))\n",
    "\n",
    "    return metric_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c4554da",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_df = pd.DataFrame(studies_list)\n",
    "\n",
    "experiment_df['alg_metrics'] = experiment_df['ml_approaches'].apply(get_metrics_experiment)\n",
    "experiment_df = experiment_df.explode('alg_metrics').reset_index(drop=True)\n",
    "experiment_df['algorithm'] = experiment_df['alg_metrics'].apply(lambda x: x[0])\n",
    "experiment_df['metrics'] = experiment_df['alg_metrics'].apply(lambda x: x[1])\n",
    "experiment_df['validation'] = experiment_df['alg_metrics'].apply(lambda x: x[2])\n",
    "experiment_df = experiment_df.drop(columns=['alg_metrics'])\n",
    "experiment_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f06749b",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_df['metric_used'] = experiment_df['metrics'].apply(lambda x: list(set(list(x.keys()) if isinstance(x, dict) else [])))\n",
    "experiment_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3de4221",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_df['validation'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "312d7841",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_val_df = experiment_df.groupby(by=['doi', 'problem'], as_index=False).agg({\n",
    "    'metric_used': list,\n",
    "    'validation': list,\n",
    "})\n",
    "metric_val_df['metrics_used'] = metric_val_df['metric_used'].apply(lambda x: set([item for sublist in x for item in sublist]))\n",
    "metric_val_df['num_metrics'] = metric_val_df['metrics_used'].apply(lambda x: len(x))\n",
    "metric_val_df['validation'] = metric_val_df['validation'].apply(lambda x: list(set(x)))\n",
    "metric_val_df['validation'] = metric_val_df['validation'].apply(lambda x: [val for val in x if val != ''])\n",
    "metric_val_df['validation'] = metric_val_df['validation'].apply(lambda x: [val for val in x if val is not None])\n",
    "metric_val_df['num_validation'] = metric_val_df['validation'].apply(lambda x: len(x))\n",
    "metric_val_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40615291",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_validation = metric_val_df.groupby(by='doi', as_index=False).agg({\n",
    "    'num_validation': 'sum'})\n",
    "num_validation['num_validation'].value_counts(normalize=True)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a6572b",
   "metadata": {},
   "outputs": [],
   "source": [
    "vals_df = metric_val_df.groupby(by='doi', as_index=False).agg({\n",
    "    'validation': list})\n",
    "vals_df['validation_used'] = vals_df['validation'].apply(lambda x: list(set([item for sublist in x for item in sublist])))\n",
    "vals_df['num_validation_used'] = vals_df['validation_used'].apply(lambda x: len(x))\n",
    "vals_df = vals_df.drop(columns=['validation'], inplace=False)\n",
    "vals_df['num_validation_used'].value_counts(normalize=True)*100\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d9c3a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df = vals_df.explode('validation_used').reset_index(drop=True)\n",
    "temp = temp_df['validation_used'].value_counts(normalize=True).reset_index()\n",
    "temp.columns = ['validation', 'proportion']\n",
    "temp['proportion'] = np.round(temp['proportion']*100, 2)\n",
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a5541fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_metrics = metric_val_df.groupby(by='doi', as_index=False).agg({\n",
    "    'num_metrics': 'sum'})\n",
    "\n",
    "print(f\"Max number of metrics in a paper: {num_metrics['num_metrics'].max()}\")\n",
    "print(f\"Min number of metrics in a paper: {num_metrics['num_metrics'].min()}\")\n",
    "print(f\"Median number of metrics in a paper: {num_metrics['num_metrics'].median():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba04e9b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_metrics[num_metrics['num_metrics'] == num_metrics['num_metrics'].min()].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56de5cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "38/260"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e27c4234",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "voice-db-M3sIqRqH-py3.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
